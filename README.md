# UI Test Generator

An AI agent that generates UI test cases for a **Grafana-based monitoring platform**, using the **TaskTracker** API to manage test cases. The agent interprets natural language requirements (e.g. *"generate tests in folder 'abyss datasource' similar to tests in folder 'postgres datasource'"*) and uses tools to fetch source tests and create new ones in the target folder.

## Tech stack

- **Python 3.11+**
- **LangGraph** – agent orchestration (ReAct-style loop with tools)
- **LangChain** – tools, chat models (OpenAI / Anthropic)
- **TaskTracker** – external API for test case CRUD (get, create, update, delete)

## Project layout

```
ui-test-generator/
├── src/
│   ├── config.py           # Env config (TaskTracker URL, API key)
│   ├── main.py             # CLI entrypoint
│   ├── agent/
│   │   ├── graph.py        # create_agent(), run_agent()
│   │   └── prompts.py      # System prompt for the agent
│   └── tasktracker/
│       ├── client.py       # HTTP client for TaskTracker API
│       ├── stub.py         # In-memory stub (same interface as client)
│       └── tools.py       # LangChain tools: get/create/update/delete test cases
├── tests/                  # Unit tests (use stub via conftest)
├── stub_servers/            # REST stubs for external services
│   └── tasktracker/         # TaskTracker API stub (folders + test cases)
├── pyproject.toml          # Project and dependencies (uv)
├── .python-version         # Python version for uv (optional)
├── uv.lock                 # Lock file (generated by uv sync)
├── .env.example
├── AGENTS.md               # Project intentions and agent behavior
└── README.md
```

## Setup

This project uses [uv](https://docs.astral.sh/uv/) for Python and dependency management.

1. **Install uv** (if needed): <https://docs.astral.sh/uv/getting-started/installation/>

2. **Create a virtualenv and install dependencies**

   ```bash
   uv sync
   ```

   This creates `.venv` and installs dependencies from `pyproject.toml` (and writes `uv.lock`).

3. **Activate the environment** (optional; `uv run` uses it automatically)

   ```bash
   .venv\Scripts\activate   # Windows
   source .venv/bin/activate  # macOS/Linux
   ```

4. **Configure environment**

   Copy `.env.example` to `.env` and set:

   - `TASKTRACKER_BASE_URL` – TaskTracker API base URL (e.g. `https://tasktracker.example.com/api`)
   - `TASKTRACKER_API_KEY` – API key if required
   - `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` for the LLM

## Usage

Run the agent with `uv run` (uses `.venv` and locked dependencies):

**One-off request**

```bash
uv run python -m src.main "Generate tests in folder 'abyss datasource' that are similar to tests in folder 'postgres datasource'"
```

**Interactive**

```bash
uv run python -m src.main -i
```

**Custom model**

```bash
uv run python -m src.main -m "anthropic:claude-3-5-sonnet-20241022" "Generate tests in 'abyss datasource' like 'postgres datasource'"
```

**Debug (output state as JSON)**

```bash
uv run python -m src.main --json "List test cases in folder 'postgres datasource'"
```

With an activated `.venv`, you can also run `python -m src.main ...` directly.

## TaskTracker API assumptions

The client in `src/tasktracker/client.py` assumes REST endpoints such as:

- `GET /test-cases?folder=<folder>` – list test cases in a folder
- `POST /test-cases` – body: `{ "folder": "<folder>", ...testCase }`
- `PUT /test-cases/<id>` – update test case
- `DELETE /test-cases/<id>` – delete test case

Adjust URLs and request/response shapes in `client.py` to match your real TaskTracker API.

**Local stub:** Set `TASKTRACKER_USE_STUB=true` in `.env` to use an in-memory stub instead of the real API (no HTTP). Unit tests always use the stub via `tests/conftest.py`. For a **REST stub server** (folders + test cases), see `stub_servers/README.md` and run `uv run uvicorn stub_servers.tasktracker.app:app --reload --port 8000`.

## Tests

```bash
uv run pytest tests/ -v
```

Tests use the TaskTracker stub automatically (no real API or env vars required).

## Agent behavior

The agent is instructed to:

1. Identify **source** and **target** folders from the user message.
2. Call **get_test_cases(source_folder)** to load existing tests.
3. For each (or a subset), derive new test case JSON adapted to the target (e.g. datasource name, queries).
4. Call **create_test_case(target_folder, test_case_json)** for each new test.

See `AGENTS.md` for full intentions and design notes.
